{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPkGAFenDhEaWypATxdK/o3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mechanics-Mechatronics-and-Robotics/ML-2025a/blob/main/Week_02/Week_02_Lab_Linear_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 02 Lab: Linear models\n",
        "This notebook was developed using methodologies suggested by ChatGPT (OpenAI, 2025)"
      ],
      "metadata": {
        "id": "AOpKnoaRdaK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro\n",
        "\n",
        "In this lab, we will learn how **linear models** work by starting from scratch.\n",
        "\n",
        "- First, we will use **NumPy** to manually implement gradient descent for **linear regression**.\n",
        "- Then, we will progressively use **PyTorch** and later **PyTorch Lightning** to make our life easier.\n",
        "- Finally, we will extend from **linear regression** to **logistic regression** for classification.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0GVDKAounTeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 1: Gradient descent implemented manually\n",
        "**Goal:** Fit a line `y = wx + b` to some toy data using gradient descent, without any ML libraries.  \n",
        "We will:\n",
        "1. Create toy data `(X, Y)`.\n",
        "2. Define a prediction function.\n",
        "3. Define Mean Squared Error (MSE) loss.\n",
        "4. Compute gradients **manually**.\n",
        "5. Update parameters step by step.\n",
        "6. Visualize the results.\n",
        "\n",
        "This shows how training actually works under the hood."
      ],
      "metadata": {
        "id": "K9NuhdFNnv3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Generate Date"
      ],
      "metadata": {
        "id": "zyGzsdPRoGjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate toy data: Y = 2*X + noise\n",
        "X = np.linspace(-3, 3, 100)\n",
        "noise = np.random.randn(100) * 0.5\n",
        "Y = 2 * X + noise\n",
        "\n",
        "# Visualize data\n",
        "plt.scatter(X, Y, alpha=0.7)\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Toy Data: Y = 2X + noise\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G1QN3K2foCBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2, 3: Initialize Model Parameters and the model itself"
      ],
      "metadata": {
        "id": "XYUOi94AobuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize parameters randomly\n",
        "w = np.random.randn()\n",
        "b = np.random.randn()\n",
        "\n",
        "print(\"Initial parameters: w =\", w, \"b =\", b)\n"
      ],
      "metadata": {
        "id": "QKipZrxooKNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x, w, b):\n",
        "    \"\"\"Linear model prediction: y = wx + b\"\"\"\n",
        "    return w * x + b\n",
        "\n",
        "# Test prediction\n",
        "print(\"Prediction for x=1.0:\", predict(1.0, w, b))\n"
      ],
      "metadata": {
        "id": "wnJeIEeBoB-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Define Objective (MSE)"
      ],
      "metadata": {
        "id": "kSaT8V-Po7_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_loss(y_true, y_pred):\n",
        "    \"\"\"Mean Squared Error\"\"\"\n",
        "    # TODO: implement formula: mean((y_true - y_pred)**2)\n",
        "    loss = None  # add your code here\n",
        "    return loss\n",
        "\n",
        "# Test loss function\n",
        "y_pred_test = predict(X, w, b)\n",
        "print(\"Initial loss:\", mse_loss(Y, y_pred_test))\n"
      ],
      "metadata": {
        "id": "_43hj_unoB78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Compute Gradient"
      ],
      "metadata": {
        "id": "zufghrcXzKRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradients(x, y_true, y_pred):\n",
        "    \"\"\"Compute gradients of loss wrt w and b\"\"\"\n",
        "    N = len(x)\n",
        "    error = y_true - y_pred\n",
        "    # TODO: implement gradients\n",
        "    dw = None  # add your code here\n",
        "    db = None  # add your code here\n",
        "    return dw, db\n"
      ],
      "metadata": {
        "id": "sNolp3XVoBkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Training Loop"
      ],
      "metadata": {
        "id": "a4XzLOAP2bgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "lr = 0.01   # learning rate\n",
        "epochs = 50\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass: prediction\n",
        "    y_pred = predict(X, w, b)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = mse_loss(Y, y_pred)\n",
        "    loss_history.append(loss)\n",
        "\n",
        "    # Backward pass: gradients\n",
        "    dw, db = compute_gradients(X, Y, y_pred)\n",
        "\n",
        "    # Parameter update\n",
        "    # TODO: update w and b using dw, db and learning rate\n",
        "    w = None  # add your code here\n",
        "    b = None  # add your code here\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}, w={w:.2f}, b={b:.2f}\")\n",
        "\n",
        "# Plot loss curve\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hY6mKRmjqv_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Visualization"
      ],
      "metadata": {
        "id": "INPonUYR5q8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X, Y, alpha=0.7, label=\"Data\")\n",
        "plt.plot(X, predict(X, w, b), color=\"red\", label=\"Fitted line\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Final Linear Fit with Gradient Descent\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tsv7CqyGqv2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 2: Gradient Descent Implemented with Pytorch\n",
        "\n"
      ],
      "metadata": {
        "id": "8xE3eQsqqu6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Imports and Data"
      ],
      "metadata": {
        "id": "syNg5NyC6s_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate toy data again (same as Block 1)\n",
        "torch.manual_seed(42)\n",
        "X = torch.linspace(-3, 3, 100).view(-1, 1)   # shape (100,1)\n",
        "Y = 2 * X + torch.randn(100, 1) * 0.5\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X.numpy(), Y.numpy(), alpha=0.7)\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Toy Data for PyTorch Regression\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "60zCVDi9q12T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Define Model"
      ],
      "metadata": {
        "id": "0jr9plmz7ewF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple linear model: y = wx + b\n",
        "# TODO: replace with nn.Linear(input dim, output dim)\n",
        "model = None  # add your code here"
      ],
      "metadata": {
        "id": "2U7NHpqh7iSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Define Objective and Optimizer"
      ],
      "metadata": {
        "id": "Oc6c4Rnv8Skv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Mean Squared Error loss\n",
        "# TODO: use nn.MSELoss()\n",
        "criterion = None  # add your code here\n",
        "\n",
        "# Define optimizer (e.g., SGD)\n",
        "# TODO: use optim.SGD with model parameters and learning rate\n",
        "optimizer = None  # add your code here\n"
      ],
      "metadata": {
        "id": "6QrYdZ368XjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Training Loop"
      ],
      "metadata": {
        "id": "Xj1Nq96F9Z8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(y_pred, Y)\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()   # reset gradients\n",
        "    loss.backward()         # compute gradients\n",
        "    optimizer.step()        # update weights\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        # TODO: print epoch, loss, and model parameters\n",
        "        pass  # add your code here\n",
        "\n",
        "# Plot loss curve\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.title(\"Training Loss (PyTorch)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RKgNFeMz9Y94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Visualize Final Fit"
      ],
      "metadata": {
        "id": "LMm0eMmr9iRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X.numpy(), Y.numpy(), alpha=0.7, label=\"Data\")\n",
        "plt.plot(X.numpy(), model(X).detach().numpy(), color=\"red\", label=\"Fitted line\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Final Linear Fit with PyTorch\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nF6K_b4h9mmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 3: Regression model"
      ],
      "metadata": {
        "id": "c8Azm-_R99na"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Generate Data"
      ],
      "metadata": {
        "id": "Haq4C_bWDHgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate nonlinear data: y = 0.5 * x^2 + noise\n",
        "torch.manual_seed(42)\n",
        "X = torch.linspace(-3, 3, 200).view(-1, 1)\n",
        "Y = 0.5 * (X ** 2) + torch.randn(200, 1) * 0.3\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X.numpy(), Y.numpy(), alpha=0.7)\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Nonlinear Toy Data for Regression\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jcp22QIB-Jlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Define Dataset and Dataloader"
      ],
      "metadata": {
        "id": "mpppKth7D41X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ToyDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # TODO: return (X[idx], Y[idx])\n",
        "        return None  # add your code here\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = ToyDataset(X, Y)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "qcVrsbXtEAbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Define Model, Objective, Optimizer"
      ],
      "metadata": {
        "id": "-DjnuH3SEM3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple linear model (wonâ€™t be perfect on quadratic data)\n",
        "# TODO: use nn.Linear(1, 1)\n",
        "model = None  # add your code here\n",
        "\n",
        "# MSE loss\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Optimizer\n",
        "# TODO: use SGD with lr=0.05\n",
        "optimizer = None  # add your code here\n"
      ],
      "metadata": {
        "id": "wGQ4TdY7EUcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Training Loop"
      ],
      "metadata": {
        "id": "lu8heeHlFDvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for batch_X, batch_Y in dataloader:\n",
        "        # Forward pass\n",
        "        y_pred = model(batch_X)\n",
        "\n",
        "        # Loss\n",
        "        loss = criterion(y_pred, batch_Y)\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "CIV9zIu4FCnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Visualization"
      ],
      "metadata": {
        "id": "syrUemAdFpNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot data\n",
        "plt.scatter(X.numpy(), Y.numpy(), alpha=0.7, label=\"Data\")\n",
        "\n",
        "# Model predictions\n",
        "with torch.no_grad():\n",
        "    Y_pred = model(X)\n",
        "plt.plot(X.numpy(), Y_pred.numpy(), color=\"red\", label=\"Fitted Line\")\n",
        "\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Regression with DataLoader (Linear Model)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jpCbOixXFszK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 4: Logistic Regression Model"
      ],
      "metadata": {
        "id": "slmUWyIiGYrw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install & Imports"
      ],
      "metadata": {
        "id": "oszpxpmXGjur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision pytorch-lightning -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import pytorch_lightning as pl\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "RwL8ALUjGfQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Dataset and Dataloader"
      ],
      "metadata": {
        "id": "wb2LQMb6HLFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNIST37(torch.utils.data.Dataset):\n",
        "    def __init__(self, train=True, transform=None):\n",
        "        # load MNIST\n",
        "        self.dataset = datasets.MNIST(root=\"./data\", train=train, download=True)\n",
        "\n",
        "        # filter only 3 and 7\n",
        "        mask = (self.dataset.targets == 3) | (self.dataset.targets == 7)\n",
        "        self.data = self.dataset.data[mask]\n",
        "        self.targets = self.dataset.targets[mask]\n",
        "\n",
        "        # convert labels: 3 -> 0, 7 -> 1\n",
        "        self.targets = (self.targets == 7).long()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx]\n",
        "        y = self.targets[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        return x, y\n",
        "\n"
      ],
      "metadata": {
        "id": "PcGxz2GZHQFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: x.float() / 255.0),  # scale and cast\n",
        "    transforms.Lambda(lambda x: x.view(-1))          # flatten\n",
        "])\n",
        "\n",
        "train_dataset = MNIST37(train=True, transform=transform)\n",
        "test_dataset = MNIST37(train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "8Cgmp2ojH3cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Set Training with Lightning"
      ],
      "metadata": {
        "id": "gQ5Z1omIH_aX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionModel(pl.LightningModule):\n",
        "    def __init__(self, input_dim=784):\n",
        "        super().__init__()\n",
        "        # TODO: create linear layer (input_dim -> 1)\n",
        "        self.linear = None  # add your code here\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: return raw logits (do not apply sigmoid here)\n",
        "        return None  # add your code here\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x).squeeze(1)\n",
        "        loss = self.criterion(logits, y.float())\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # TODO: return SGD optimizer with lr=0.1\n",
        "        return None  # add your code here\n"
      ],
      "metadata": {
        "id": "kuYFGmRfIZjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Define and Trein the Model"
      ],
      "metadata": {
        "id": "lODeVzntJVmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegressionModel()\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=5, accelerator=\"cpu\", log_every_n_steps=10)\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=5,\n",
        "    accelerator=\"auto\",   # auto-detects CPU/GPU\n",
        "    devices=1,            # use 1 GPU if present\n",
        ")\n",
        "\n",
        "trainer.fit(model, train_loader)"
      ],
      "metadata": {
        "id": "PFfs7PsxKavZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Visualize Predictions"
      ],
      "metadata": {
        "id": "tFNYrHpLORdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show some predictions\n",
        "examples = next(iter(test_loader))\n",
        "x, y = examples\n",
        "with torch.no_grad():\n",
        "    logits = model(x).squeeze(1)\n",
        "    preds = (torch.sigmoid(logits) > 0.5).long()\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i+1)\n",
        "    plt.imshow(x[i].view(28, 28), cmap=\"gray\")\n",
        "    plt.title(f\"Label: {y[i].item()}, Pred: {preds[i].item()}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "acc = (preds == y).float().mean()\n",
        "print('accuracy = ', acc)"
      ],
      "metadata": {
        "id": "9FhY1VUfOVdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Tasks and Assessment\n",
        "\n",
        "## ðŸŽ¯ Main Track (Bâ€“D)\n",
        "- Complete **all blocks** (1â€“4) by filling in the TODOs in the code.\n",
        "- Ensure your code runs and produces the expected **plots and metrics**:\n",
        "  - Block 1: Manual gradient descent\n",
        "  - Block 2: PyTorch gradient descent\n",
        "  - Block 3: Regression with Dataset/DataLoader\n",
        "  - Block 4: Logistic regression on MNIST (digits 3 vs 7)\n",
        "- Visualizations and logging must be present.  \n",
        "- **Bâ€“D** grades are for correctly completing the main track:\n",
        "  - **B:** All core tasks complete and correct.\n",
        "  - **C:** Most core tasks complete, minor errors.\n",
        "  - **D:** Some core tasks complete, major errors.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŒŸ Additional Tasks (for grade A)\n",
        "To earn **grade A**, complete **at least one** of the following additional tasks. You may do more for extra learning.\n",
        "\n",
        "### **Task 1: Multi-digit Classification**\n",
        "- Extend MNIST37 dataset to another pair of digits (e.g., 0 vs 1).  \n",
        "- Train a logistic regression model using Lightning.  \n",
        "- Log training/validation loss and accuracy.  \n",
        "- Optional: Visualize a **confusion matrix** of predictions.\n",
        "\n",
        "### **Task 2: Mini-batch Gradient Descent (Manual)**\n",
        "- Implement **mini-batch gradient descent** on the toy dataset from Block 1.  \n",
        "- Compare your loss curve with PyTorch training (Block 2).  \n",
        "- âœ… Goal: Connect **manual GD and library-powered training**.\n",
        "\n",
        "### **Task 3: Data Noise Exploration**\n",
        "- In Block 1 or 3, increase the noise in the target variable `Y`.  \n",
        "- Observe how gradient descent behaves on noisy data.   \n",
        "\n",
        "### **Task 4: Visualization Enhancements**\n",
        "- Add validation step into LogisticRegressionModel()\n",
        "- Add plots showing **loss vs epochs** for both training and validation (if applicable).  \n",
        "- Visualize **decision boundary** of logistic regression (optional for 2D projections).  \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Š Assessment Criteria\n",
        "\n",
        "| Grade | Criteria |\n",
        "|-------|---------|\n",
        "| **A** | Successfully completes **at least one additional task** **and** main track completed correctly. |\n",
        "| **B** | Completes all **main track tasks** correctly (Blocks 1â€“4) and plots/logging present. |\n",
        "| **C** | Completes most main track tasks with minor errors. |\n",
        "| **D** | Completes some main track tasks; significant errors or missing code. |\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ’¡ Notes\n",
        "- Include **comments in code** explaining what changes you made for each task.  \n",
        "- For optional tasks, plots or metrics are strongly encouraged to visualize outcomes.  \n",
        "- Extra effort beyond A is encouraged, but only one additional task is needed for grade A.\n",
        "\n"
      ],
      "metadata": {
        "id": "mqwlbX8nRCiy"
      }
    }
  ]
}