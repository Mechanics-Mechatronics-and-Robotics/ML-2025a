{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMwn9ehG2TtuPZsvX/w7qfF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mechanics-Mechatronics-and-Robotics/ML-2025a/blob/main/Week_02/Week_02_Lab_Linear_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 02 Lab: Linear models\n",
        "This notebook was developed using methodologies suggested by ChatGPT (OpenAI, 2025)"
      ],
      "metadata": {
        "id": "AOpKnoaRdaK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro\n",
        "\n",
        "In this lab, we will learn how **linear models** work by starting from scratch.\n",
        "\n",
        "- First, we will use **NumPy** to manually implement gradient descent for **linear regression**.\n",
        "- Then, we will progressively use **PyTorch** and later **PyTorch Lightning** to make our life easier.\n",
        "- Finally, we will extend from **linear regression** to **logistic regression** for classification.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0GVDKAounTeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 1: Gradient descent implemented manually\n",
        "**Goal:** Fit a line `y = wx + b` to some toy data using gradient descent, without any ML libraries.  \n",
        "We will:\n",
        "1. Create toy data `(X, Y)`.\n",
        "2. Define a prediction function.\n",
        "3. Define Mean Squared Error (MSE) loss.\n",
        "4. Compute gradients **manually**.\n",
        "5. Update parameters step by step.\n",
        "6. Visualize the results.\n",
        "\n",
        "This shows how training actually works under the hood."
      ],
      "metadata": {
        "id": "K9NuhdFNnv3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Generate Date"
      ],
      "metadata": {
        "id": "zyGzsdPRoGjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate toy data: Y = 2*X + noise\n",
        "X = np.linspace(-3, 3, 100)\n",
        "noise = np.random.randn(100) * 0.5\n",
        "Y = 2 * X + noise\n",
        "\n",
        "# Visualize data\n",
        "plt.scatter(X, Y, alpha=0.7)\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Toy Data: Y = 2X + noise\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G1QN3K2foCBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2, 3: Initialize Model Parameters and the model itself"
      ],
      "metadata": {
        "id": "XYUOi94AobuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize parameters randomly\n",
        "w = np.random.randn()\n",
        "b = np.random.randn()\n",
        "\n",
        "print(\"Initial parameters: w =\", w, \"b =\", b)\n"
      ],
      "metadata": {
        "id": "QKipZrxooKNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x, w, b):\n",
        "    \"\"\"Linear model prediction: y = wx + b\"\"\"\n",
        "    return w * x + b\n",
        "\n",
        "# Test prediction\n",
        "print(\"Prediction for x=1.0:\", predict(1.0, w, b))\n"
      ],
      "metadata": {
        "id": "wnJeIEeBoB-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Define Objective (MSE)"
      ],
      "metadata": {
        "id": "kSaT8V-Po7_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_loss(y_true, y_pred):\n",
        "    \"\"\"Mean Squared Error\"\"\"\n",
        "    # TODO: implement formula: mean((y_true - y_pred)**2)\n",
        "    loss = None  # add your code here\n",
        "    return loss\n",
        "\n",
        "# Test loss function\n",
        "y_pred_test = predict(X, w, b)\n",
        "print(\"Initial loss:\", mse_loss(Y, y_pred_test))\n"
      ],
      "metadata": {
        "id": "_43hj_unoB78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Compute Gradient"
      ],
      "metadata": {
        "id": "zufghrcXzKRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradients(x, y_true, y_pred):\n",
        "    \"\"\"Compute gradients of loss wrt w and b\"\"\"\n",
        "    N = len(x)\n",
        "    error = y_true - y_pred\n",
        "    # TODO: implement gradients\n",
        "    dw = None  # add your code here\n",
        "    db = None  # add your code here\n",
        "    return dw, db\n"
      ],
      "metadata": {
        "id": "sNolp3XVoBkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Training Loop"
      ],
      "metadata": {
        "id": "a4XzLOAP2bgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "lr = 0.01   # learning rate\n",
        "epochs = 50\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass: prediction\n",
        "    y_pred = predict(X, w, b)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = mse_loss(Y, y_pred)\n",
        "    loss_history.append(loss)\n",
        "\n",
        "    # Backward pass: gradients\n",
        "    dw, db = compute_gradients(X, Y, y_pred)\n",
        "\n",
        "    # Parameter update\n",
        "    # TODO: update w and b using dw, db and learning rate\n",
        "    w = None  # add your code here\n",
        "    b = None  # add your code here\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}, w={w:.2f}, b={b:.2f}\")\n",
        "\n",
        "# Plot loss curve\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hY6mKRmjqv_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Visualization"
      ],
      "metadata": {
        "id": "INPonUYR5q8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X, Y, alpha=0.7, label=\"Data\")\n",
        "plt.plot(X, predict(X, w, b), color=\"red\", label=\"Fitted line\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Final Linear Fit with Gradient Descent\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tsv7CqyGqv2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 2: Gradient Descent Implemented with Pytorch\n",
        "\n"
      ],
      "metadata": {
        "id": "8xE3eQsqqu6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Imports and Data"
      ],
      "metadata": {
        "id": "syNg5NyC6s_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate toy data again (same as Block 1)\n",
        "torch.manual_seed(42)\n",
        "X = torch.linspace(-3, 3, 100).view(-1, 1)   # shape (100,1)\n",
        "Y = 2 * X + torch.randn(100, 1) * 0.5\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X.numpy(), Y.numpy(), alpha=0.7)\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Toy Data for PyTorch Regression\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "60zCVDi9q12T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Define Model"
      ],
      "metadata": {
        "id": "0jr9plmz7ewF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple linear model: y = wx + b\n",
        "# TODO: replace with nn.Linear(input dim, output dim)\n",
        "model = None  # add your code here"
      ],
      "metadata": {
        "id": "2U7NHpqh7iSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Define Objective and Optimizer"
      ],
      "metadata": {
        "id": "Oc6c4Rnv8Skv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Mean Squared Error loss\n",
        "# TODO: use nn.MSELoss()\n",
        "criterion = None  # add your code here\n",
        "\n",
        "# Define optimizer (e.g., SGD)\n",
        "# TODO: use optim.SGD with model parameters and learning rate\n",
        "optimizer = None  # add your code here\n"
      ],
      "metadata": {
        "id": "6QrYdZ368XjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Training Loop"
      ],
      "metadata": {
        "id": "Xj1Nq96F9Z8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(y_pred, Y)\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()   # reset gradients\n",
        "    loss.backward()         # compute gradients\n",
        "    optimizer.step()        # update weights\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        # TODO: print epoch, loss, and model parameters\n",
        "        pass  # add your code here\n",
        "\n",
        "# Plot loss curve\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.title(\"Training Loss (PyTorch)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RKgNFeMz9Y94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Visualize Final Fit"
      ],
      "metadata": {
        "id": "LMm0eMmr9iRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X.numpy(), Y.numpy(), alpha=0.7, label=\"Data\")\n",
        "plt.plot(X.numpy(), model(X).detach().numpy(), color=\"red\", label=\"Fitted line\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Final Linear Fit with PyTorch\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nF6K_b4h9mmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 3: Regression"
      ],
      "metadata": {
        "id": "c8Azm-_R99na"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jcp22QIB-Jlp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}